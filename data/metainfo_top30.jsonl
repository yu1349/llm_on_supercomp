{"text": "[SectionTitle] 4 Composition in a K&G Parser [Body] Parser We use UUParser, a variant of the K&G transition-based parser that employs the arc-hybrid transition system from Kuhlmann et al. (2011) extended with a S WAP transition and a Static-Dynamic oracle, as described in de Lhoneux et al. (2017b) [Cite_Footnote_4] . The S WAP transition is used to allow the construction of non-projective dependency trees (Nivre, 2009). We use default hyperparameters. When using POS tags, we use the universal POS tags from the UD treebanks which are coarse-grained and consistent across languages. Those POS tags are predicted by UDPipe (Straka et al., 2016) both for training and parsing. This parser obtained the 7th best LAS score on average in the 2018 CoNLL shared task (Zeman et al., 2018), about 2.5 LAS points below the best system, which uses an ensemble system as well as ELMo embed-dings, as introduced by Peters et al. (2018). Note, however, that we use a slightly impoverished ver-sion of the model used for the shared task which is described in Smith et al. (2018a): we use a less ac-curate POS tagger (UDPipe) and we do not make use of multi-treebank models. In addition, Smith et al. (2018a) use the three top items of the stack as well as the first item of the buffer to represent the configuration, while we only use the two top items of the stack and the first item of the buffer. Smith et al. (2018a) also use an extended feature set as introduced by Kiperwasser and Goldberg (2016b) where they also use the rightmost and left-most children of the items of the stack and buffer that they consider. We do not use that extended feature set. This is to keep the parser settings as simple as possible and avoid adding confounding factors. It is still a near-SOTA model. We evaluate parsing models on the development sets and report the average of the 5 best results in 30 epochs and 5 runs with different random seeds. [FootnoteReference] 4 The code can be found at https://github.com/mdelhoneux/uuparser-composition", "char_spans": [[1971, 2021, "URL"], [1946, 1954, "Generic mention"], [273, 298, "Citation tag"], [66, 74, "Name"], [76, 298, "Description"]], "id": 0, "role": "Method", "type": "Code", "function": "Use", "url": "https://github.com/mdelhoneux/uuparser-composition", "name": ["UUParser"], "fullname": ["N/A"], "genericmention": ["The code"], "citationtag": ["de Lhoneux et al. (2017b)"], "description": ["a variant of the K&G transition-based parser that employs the arc-hybrid transition system from Kuhlmann et al. (2011) extended with a S WAP transition and a Static-Dynamic oracle, as described in de Lhoneux et al. (2017b)"]}
{"text": "[SectionTitle] 5 What Correlates with Difficulty? [Body] Head-POS Entropy Dehouck and Denis (2018) propose an alternative measure of morphosyntactic complexity. Given a corpus of dependency graphs, they estimate the conditional entropy of the POS tag of a random token\u2019s parent, conditioned on the token\u2019s type. In a language where this HPE-mean metric is low, most tokens can predict the POS of their parent even without context. We compute HPE-mean from dependency parses of the Europarl data, generated using UDPipe 1.2.0 (Straka et al., 2016) and freely-available tokenization, tagging, parsing models trained on the Universal Depen-dencies 2.0 treebanks (Straka and Strakov, 2017)  . [FootnoteReference] Milan Straka and Jana Strakov. 2017. Tokenizing, POS tagging, lemmatizing and parsing UD 2.0 with UDPipe. In CoNLL 2017 Shared Task: Multilin-gual parsing from raw text to Universal Dependen-cies, pages 88\u201399. Documented models at http://hdl.handle.net/11234/1-2364.", "char_spans": [[940, 974, "URL"], [919, 936, "Generic mention"], [551, 685, "Description"], [659, 685, "Citation tag"]], "id": 1, "role": "Material", "type": "Knowledge", "function": "Use", "url": "http://hdl.handle.net/11234/1-2364", "name": ["N/A"], "fullname": ["N/A"], "genericmention": ["Documented models"], "citationtag": ["(Straka and Strakov, 2017)"], "description": ["freely-available tokenization, tagging, parsing models trained on the Universal Depen-dencies 2.0 treebanks (Straka and Strakov, 2017)"]}
{"text": "[SectionTitle] D Data selection: Europarl [Body] Finally, it should be said that the text in CoStEP itself contains some markup, marking reports, el-lipses, etc., but we strip this additional markup to obtain the raw text. We tokenize it using the re-versible language-agnostic tokenizer of Mielke and Eisner (2018) [Cite_Footnote_31] and split the obtained 78169 para-graphs into training set, development set for tuning our language models, and test set for our regres-sion, again by dividing the data into blocks of 30 paragraphs and then taking 5 sentences for the de-velopment and test set each, leaving the remainder for the training set. This way we ensure uniform division over sessions of the parliament and sizes of 2 / 3 , 1 / 6 , and 1 / 6 , respectively. [FootnoteReference] 31 http://sjmielke.com/papers/tokenize/", "char_spans": [[791, 827, "URL"], [291, 315, "Citation tag"], [244, 315, "Description"]], "id": 2, "role": "Method", "type": "Tool", "function": "Use", "url": "http://sjmielke.com/papers/tokenize/", "name": ["N/A"], "fullname": ["N/A"], "genericmention": ["N/A"], "citationtag": ["Mielke and Eisner (2018)"], "description": ["the re-versible language-agnostic tokenizer of Mielke and Eisner (2018)"]}
{"text": "[SectionTitle] 2 Problem Formulation 2.2 Data [Body] \u2022 Negative examples: We have col-lected 1% of tweets from Twitter\u2019s daily feed using the Twitter API (  https://developer.twitter.com/en/docs.html) to use as negative examples.", "char_spans": [[157, 199, "URL"], [142, 153, "Name"]], "id": 3, "role": "Method", "type": "Tool", "function": "Use", "url": "https://developer.twitter.com/en/docs.html", "name": ["Twitter API"], "fullname": ["N/A"], "genericmention": ["N/A"], "citationtag": ["N/A"], "description": ["N/A"]}
{"text": "[SectionTitle] 5 User study [Body] To verify whether human evaluators are in agree-ment with our characterization model, we con-ducted a user study using MTurk (Amazon, 2005)  . [FootnoteReference] Amazon. 2005. MTurk. (https://www.mturk.com/).", "char_spans": [[220, 242, "URL"], [160, 175, "Citation tag"], [154, 159, "Name"]], "id": 4, "role": "Method", "type": "Tool", "function": "Use", "url": "https://www.mturk.com/", "name": ["MTurk"], "fullname": ["N/A"], "genericmention": ["N/A"], "citationtag": ["(Amazon, 2005) "], "description": ["N/A"]}
{"text": "[SectionTitle] 4 Approaches to authorship verification 4.4 Approach 4: Document embeddings [Body] 1. We obtain representations of tweets as doc-ument embeddings. We experiment with two types of document embeddings: Fast-Text (Facebook-Research, 2016)  (embedding size = 100) and BERT-Base, uncased (Devlin et al., 2018) (embedding size = 768). [FootnoteReference] Facebook-Research. 2016. FastText. (https://research.fb.com/fasttext/).", "char_spans": [[400, 433, "URL"], [226, 251, "Citation tag"], [214, 224, "Name"], [194, 224, "Description"]], "id": 5, "role": "Method", "type": "Code", "function": "Use", "url": "https://research.fb.com/fasttext/", "name": [" Fast-Text"], "fullname": ["N/A"], "genericmention": ["N/A"], "citationtag": ["Facebook-Research, 2016) "], "description": ["document embeddings: Fast-Text"]}
{"text": "[SectionTitle] 3 The Proposed Method 3.1 Automatic Seed Generation [Body] The seed set consists of positive labeled examples (i.e. product features) and negative labeled exam-ples (i.e. noise terms). Intuitively, popular product features are frequently mentioned in reviews, so they can be extracted by simply mining frequently occurring nouns (Hu and Liu, 2004). However, this strategy will also find many noise terms (e.g., commonly used nouns like thing, one, etc.). To produce high quality seeds, we employ a Domain Relevance Measure (DRM) (Jiang and Tan, 2010), which combines term frequency with a domain-specific measuring metric called Likelihood Ratio Test (LRT) (Dunning, 1993). Let \u03bb(t) denotes the LRT score of a product feature candidate t, where k 1 and k 2 are the frequencies of t in the review corpus R and a background corpus [Cite_Footnote_1] B, n 1 and n 2 are the total number of terms in R and B, p = (k 1 + k 2 )/(n 1 + n 2 ), p 1 = k 1 /n 1 and p 2 = k 2 /n 2 . Then a modified DRM 2 is proposed, where tf(t) is the frequency of t in R and df(t) is the frequency of t in B. [FootnoteReference] 1 Google-n-Gram (http://books.google.com/ngrams) is used as the background corpus.", "char_spans": [[1135, 1165, "URL"], [689, 983, "Description"], [1120, 1133, "Name"]], "id": 6, "role": "Material", "type": "Knowledge", "function": "Use", "url": "http://books.google.com/ngrams", "name": ["Google-n-Gram"], "fullname": ["N/A"], "genericmention": ["N/A"], "citationtag": ["N/A"], "description": ["Let \u03bb(t) denotes the LRT score of a product feature candidate t, where k 1 and k 2 are the frequencies of t in the review corpus R and a background corpus [Cite_Footnote_1] B, n 1 and n 2 are the total number of terms in R and B, p = (k 1 + k 2 )/(n 1 + n 2 ), p 1 = k 1 /n 1 and p 2 = k 2 /n 2"]}
{"text": "[SectionTitle] 3 The Proposed Method 3.2 Capturing Lexical Semantic Clue in a Semantic Similarity Graph 3.2.1 Learning Word Embedding for [Body] To alleviate the data sparsity problem, EB is first trained on a very large corpus [Cite_Footnote_3] (denoted by C), and then fine-tuned on the target review cor-pus R. Particularly, for phrasal product features, a statistic-based method in (Zhu et al., 2009) is used to detect noun phrases in R. Then, an Unfold-ing Recursive Autoencoder (Socher et al., 2011) is trained on C to obtain embedding vectors for noun phrases. In this way, semantics of infrequent terms in R can be well captured. Finally, the phrase-based Skip-gram model in (Mikolov et al., 2013) is applied on R. [FootnoteReference] 3 Wikipedia(http://www.wikipedia.org) is used in practice.", "char_spans": [[755, 779, "URL"], [745, 754, "Name"], [208, 227, "Generic mention"]], "id": 7, "role": "Material", "type": "Knowledge", "function": "Use", "url": "http://www.wikipedia.org", "name": ["Wikipedia"], "fullname": ["N/A"], "genericmention": ["a very large corpus"], "citationtag": ["N/A"], "description": ["N/A"]}
{"text": "[SectionTitle] 4 Experiments 4.1 Datasets and Evaluation Metrics [Body] Datasets: We select two real world datasets to evaluate the proposed method. The first one is a benchmark dataset in Wang et al. (2011), which contains English review sets on two do-mains (MP3 and Hotel) [Cite_Footnote_5] . The second dataset is proposed by Chinese Opinion Analysis Evalua-tion 2008 (COAE 2008) , where two review sets (Camera and Car) are selected. Xu et al. (2013) had manually annotated product features on these four domains, so we directly employ their annota-tion as the gold standard. The detailed information can be found in their original paper. [FootnoteReference] 5 http://timan.cs.uiuc.edu/downloads.html", "char_spans": [[666, 705, "URL"], [149, 275, "Description"], [189, 207, "Citation tag"]], "id": 8, "role": "Method", "type": "Code", "function": "Use", "url": "http://timan.cs.uiuc.edu/downloads.html", "name": ["N/A"], "fullname": ["N/A"], "genericmention": ["N/A"], "citationtag": ["Wang et al. (2011)"], "description": ["The first one is a benchmark dataset in Wang et al. (2011), which contains English review sets on two do-mains (MP3 and Hotel)"]}
{"text": "[SectionTitle] 4 Experiments 4.1 Datasets and Evaluation Metrics [Body] Datasets: We select two real world datasets to evaluate the proposed method. The first one is a benchmark dataset in Wang et al. (2011), which contains English review sets on two do-mains (MP3 and Hotel) . The second dataset is proposed by Chinese Opinion Analysis Evalua-tion 2008 (COAE 2008) [Cite_Footnote_6] , where two review sets (Camera and Car) are selected. Xu et al. (2013) had manually annotated product features on these four domains, so we directly employ their annota-tion as the gold standard. The detailed information can be found in their original paper. [FootnoteReference] 6 http://ir-china.org.cn/coae2008.html", "char_spans": [[666, 702, "URL"], [278, 437, "Description"], [355, 364, "Name"], [312, 354, "Full name"]], "id": 9, "role": "Method", "type": "Code", "function": "Use", "url": "http://ir-china.org.cn/coae2008.html", "name": ["COAE 2008"], "fullname": ["Chinese Opinion Analysis Evalua-tion 2008 "], "genericmention": ["N/A"], "citationtag": ["N/A"], "description": ["The second dataset is proposed by Chinese Opinion Analysis Evalua-tion 2008 (COAE 2008) [Cite_Footnote_6] , where two review sets (Camera and Car) are selected"]}
{"text": "[SectionTitle] 4 Experiments and Results 4.1 Experiment Design [Body] For empirical comparison with SVM and bootstrap-ping, we evaluated LP on widely used benchmark corpora - \u201cinterest\u201d, \u201cline\u201d [Cite_Footnote_1] and the data in English lexical sample task of SENSEVAL-3 (including all 57 English words ) . from 1% to 100%. The lower table lists the official result of baseline (using most frequent sense heuristics) and top 3 sys-tems in ELS task of SENSEVAL-3. [FootnoteReference] 1 Available at http://www.d.umn.edu/\u223ctpederse/data.html", "char_spans": [[497, 537, "URL"], [143, 194, "Description"]], "id": 10, "role": "Material", "type": "Dataset", "function": "Compare", "url": "http://www.d.umn.edu/\u223ctpederse/data.html", "name": ["N/A"], "fullname": ["N/A"], "genericmention": ["N/A"], "citationtag": ["N/A"], "description": ["widely used benchmark corpora - \u201cinterest\u201d, \u201cline\u201d "]}
{"text": "[SectionTitle] 4 Experiments and Results 4.1 Experiment Design [Body] For empirical comparison with SVM and bootstrap-ping, we evaluated LP on widely used benchmark corpora - \u201cinterest\u201d, \u201cline\u201d and the data in English lexical sample task of SENSEVAL-3 (including all 57 English words ) [Cite_Footnote_2] . from 1% to 100%. The lower table lists the official result of baseline (using most frequent sense heuristics) and top 3 sys-tems in ELS task of SENSEVAL-3. [FootnoteReference] 2 Available at http://www.senseval.org/senseval3", "char_spans": [[497, 530, "URL"], [241, 251, "Name"], [198, 321, "Description"]], "id": 11, "role": "Material", "type": "Dataset", "function": "Compare", "url": "http://www.senseval.org/senseval3", "name": ["SENSEVAL-3"], "fullname": ["N/A"], "genericmention": ["N/A"], "citationtag": ["N/A"], "description": ["the data in English lexical sample task of SENSEVAL-3 (including all 57 English words ) [Cite_Footnote_2] . from 1% to 100%"]}
{"text": "[SectionTitle] 4 Experiments and Results 4.2 Experiment 1: LP vs. SVM [Body] Table 1 reports the average accuracies and paired t-test results of SVM and LP with different sizes of labled data. It also lists the official results of baseline method and top [Cite_Footnote_3] systems in ELS task of SENSEVAL-3. [FootnoteReference] 3 we SV M light ,used linear available at http://svmlight.joachims.org/.", "char_spans": [[370, 399, "URL"], [193, 306, "Description"], [333, 343, "Name"]], "id": 12, "role": "Method", "type": "Tool", "function": "Introduce", "url": "http://svmlight.joachims.org/", "name": ["SV M light"], "fullname": ["N/A"], "genericmention": ["N/A"], "citationtag": ["N/A"], "description": ["It also lists the official results of baseline method and top [Cite_Footnote_3] systems in ELS task of SENSEVAL-3"]}
{"text": "[SectionTitle] 4 Experiments and Results 4.4 An Example: Word \u201cuse\u201d [Body] For investigating the reason for LP to outperform SVM and monolingual bootstrapping, we used the data of word \u201cuse\u201d in English lexical sample task of SENSEVAL-3 as an example (totally 26 examples in training set and 14 examples in test set). For data visualization, we conducted unsupervised nonlinear dimensionality reduction [Cite_Footnote_5] on these 40 feature vec-tors with 210 dimensions. Figure 3 (a) shows the dimensionality reduced vectors in two-dimensional space. We randomly sampled only one labeled ex-ample for each sense of word \u201cuse\u201d as labeled data. The remaining data in training set and test set served as unlabeled data for bootstrapping and LP. All of these three algorithms are evaluated using accuracy on test set. [FootnoteReference] 5 We used Isomap to perform dimensionality reduction by computing two-dimensional, 39-nearest-neighbor-preserving embedding of 210-dimensional input. Isomap is available at http://isomap.stanford.edu/.", "char_spans": [[1006, 1033, "URL"], [983, 989, "Name"], [354, 468, "Description"]], "id": 13, "role": "Method", "type": "Code", "function": "Use", "url": "http://isomap.stanford.edu/", "name": ["Isomap"], "fullname": ["N/A"], "genericmention": ["N/A"], "citationtag": ["N/A"], "description": ["unsupervised nonlinear dimensionality reduction [Cite_Footnote_5] on these 40 feature vec-tors with 210 dimensions"]}
{"text": "[SectionTitle] References [Body] Due to their inherent capability in semantic alignment of aspects and their context words, attention mechanism and Convolutional Neu-ral Networks (CNNs) are widely applied for aspect-based sentiment classification. How-ever, these models lack a mechanism to ac-count for relevant syntactical constraints and long-range word dependencies, and hence may mistakenly recognize syntactically irrelevant contextual words as clues for judging aspect sentiment. To tackle this problem, we pro-pose to build a Graph Convolutional Network (GCN) over the dependency tree of a sentence to exploit syntactical information and word dependencies. Based on it, a novel aspect-specific sentiment classification framework is raised. Experiments on three benchmarking collections illustrate that our proposed model has comparable effectiveness to a range of state-of-the-art models [Cite_Footnote_1] , and further demon-strate that both syntactical information and long-range word dependencies are properly captured by the graph convolution structure. [FootnoteReference] 1 Code and preprocessed datasets are available at https://github.com/GeneZC/ASGCN.", "char_spans": [[1136, 1167, "URL"], [1088, 1118, "Generic mention"], [748, 895, "Description"]], "id": 14, "role": "Material", "type": "Dataset", "function": "Use", "url": "https://github.com/GeneZC/ASGCN", "name": ["N/A"], "fullname": ["N/A"], "genericmention": ["Code and preprocessed datasets"], "citationtag": ["N/A"], "description": ["Experiments on three benchmarking collections illustrate that our proposed model has comparable effectiveness to a range of state-of-the-art models"]}
{"text": "[SectionTitle] 3 Problem Formulation [Body] The full details of SVR and its implementation are beyond the scope of this paper; interested readers are referred to Scho\u0308lkopf and Smola (2002). SVM light (Joachims, 1999) is a freely available implementa-tion of SVR training that we used in our experi-ments. [Cite_Footnote_2] [FootnoteReference] 2 Available at http://svmlight.joachims.org.", "char_spans": [[359, 387, "URL"], [191, 200, "Name"], [202, 216, "Citation tag"], [191, 304, "Description"]], "id": 15, "role": "Method", "type": "Tool", "function": "Use", "url": "http://svmlight.joachims.org", "name": ["SVM light"], "fullname": ["N/A"], "genericmention": ["N/A"], "citationtag": ["Joachims, 1999"], "description": ["SVM light (Joachims, 1999) is a freely available implementa-tion of SVR training that we used in our experi-ments"]}
{"text": "[SectionTitle] 4 Dataset [Body] In addition to the reports, we used the Center for Research in Security Prices (CRSP) US Stocks Database to obtain the price return series along with other firm characteristics. [Cite_Footnote_4] We proceeded to calcu-late two volatilities for each firm/report observation: the twelve months prior to the report (v (\u221212) ) and the twelve months after the report (v (+12) ). [FootnoteReference] 4 The text and volatility data are publicly available at http://www.ark.cs.cmu.edu/10K.", "char_spans": [[483, 512, "URL"], [428, 456, "Generic mention"], [72, 136, "Name"]], "id": 16, "role": "Material", "type": "Dataset", "function": "Use", "url": "http://www.ark.cs.cmu.edu/10K", "name": ["Center for Research in Security Prices (CRSP) US Stocks Database"], "fullname": ["N/A"], "genericmention": ["The text and volatility data"], "citationtag": ["N/A"], "description": ["N/A"]}
{"text": "[SectionTitle] 1 Introduction [Body] This paper begins with an informal description of GMTG. It continues with an investigation of this formalism\u2019s generative capacity. Next, we prove that in GMTG each component grammar retains its generative power, a requirement for synchronous formalisms that Rambow and Satta (1996) called the \u201cweak language preservation property.\u201d Lastly, we propose a synchronous generalization of Chom-sky Normal Form, which lays the groundwork for synchronous parsing under GMTG using a CKY-style algorithm (Younger, 1967; Melamed, 2004  ). [FootnoteReference] I. Dan Melamed, G. Satta, and B. Wellington. 2004. Gener-alized multitext grammars. Technical Report 04-003, NYU Proteus Project. http://nlp.cs.nyu.edu/pubs/.", "char_spans": [[378, 561, "Description"], [533, 561, "Citation tag"], [421, 441, "Name"]], "id": 17, "role": "Method", "type": "Code", "function": "Use", "url": "we propose a synchronous generalization of Chom-sky Normal Form, which lays the groundwork for synchronous parsing under GMTG using a CKY-style algorithm (Younger, 1967; Melamed, 2004", "name": ["Chom-sky Normal Form"], "fullname": ["N/A"], "genericmention": ["N/A"], "citationtag": ["Younger, 1967; Melamed, 2004"], "description": ["we propose a synchronous generalization of Chom-sky Normal Form, which lays the groundwork for synchronous parsing under GMTG using a CKY-style algorithm (Younger, 1967; Melamed, 2004"]}
{"text": "[SectionTitle] 6 Generalized Chomsky Normal Form 6.2 Step 4: Eliminate \u2019s [Body] Grammars in GCNF cannot have \u2019s in their productions. Thus, GCNF is a more restrictive normal form than those used by Wu (1997) and Melamed (2003). The absence of \u2019s simplifies parsers for GMTG (Melamed, 2004)  . Given a GMTG u with in some productions, we give the construction of a weakly equivalent gram-mar u9O without any \u2019s. First, determine all nullable links and associated - strings in u . - A link * Z Z is nullable if < y is an ITV where at least one y\u2022bhg is . We say the link is nullable and the string at address in is nullable. For each nullable link, we create versions of the link, where is the number of nullable strings of that link. There is one version for each of the possible combinations of the nullable strings being present or absent. The version of the link with all strings present is its original version. Each non-original version of the link (except in the case of start links) gets a unique subscript, which is applied to all the nonterminals in the link, so that each link is unique in the grammar. We construct a new grammar u O whose set of productions w O is determined as follows: for each production, we identify the nullable links on the RHS and replace them with each combination of the non-original versions found earlier. If a string is left empty during this process, that string is removed from the RHS and the fan-out of the production component is reduced by one. The link on the LHS is replaced with its appropriate matching non-original link. There is one exception to the replacements. If a production consists of all nullable strings, do not include this case. Lastly, we remove all strings on the RHS of productions that have \u2019s, and reduce the fan-out of the productions accordingly. Once again, we replace the LHS link with the appropriate version. case and are nullable - so we create 54 a new version of both links: and . We then alter the productions. Pro-duction (31) gets replaced by (40). A new produc-tion based on (30) is Production (38). Lastly, Pro-duction (29) has two nullable strings on the RHS, so it gets altered to add three new productions, (34), (35) and (36). The altered set of productions are the following: [FootnoteReference] I. Dan Melamed, G. Satta, and B. Wellington. 2004. Gener-alized multitext grammars. Technical Report 04-003, NYU Proteus Project. http://nlp.cs.nyu.edu/pubs/.", "char_spans": [[2413, 2440, "URL"], [229, 290, "Description"], [275, 290, "Citation tag"]], "id": 18, "role": "Material", "type": "Knowledge", "function": "Use", "url": "http://nlp.cs.nyu.edu/pubs/", "name": ["N/A"], "fullname": ["N/A"], "genericmention": ["N/A"], "citationtag": ["(Melamed, 2004)"], "description": ["The absence of \u2019s simplifies parsers for GMTG (Melamed, 2004)"]}
{"text": "[SectionTitle] 6 Experiments and Results [Body] Everything necessary to replicate our experimen-tal results can be found in our open-source code repository. [Cite_Footnote_4] [FootnoteReference] 4 http://hohocode.github.io/textSimilarityConvNet/", "char_spans": [[197, 245, "URL"], [48, 155, "Description"], [128, 144, "Generic mention"]], "id": 19, "role": "Method", "type": "Tool", "function": "Introduce", "url": "http://hohocode.github.io/textSimilarityConvNet/", "name": ["N/A"], "fullname": ["N/A"], "genericmention": ["open-source code"], "citationtag": ["N/A"], "description": ["Everything necessary to replicate our experimen-tal results can be found in our open-source code repository"]}
{"text": "[SectionTitle] 1 Introduction [Body] In our study, we restrict the structure learning problem to predicting edges or attachments be-tween DU pairs in the dependency graph. After training a supervised deep learning algorithm to predict attachments on the STAC corpus [Cite_Footnote_1] , we then constructed a weakly supervised learning system in which we used 10% of the corpus as a develop-ment set. Experts on discourse structure wrote a set of attachment rules, Labeling Functions (LFs), with reference to this development set. Although the whole of the STAC corpus is annotated, we treated the remainder of the corpus as unseen/u-nannotated data in order to simulate the conditions in which the snorkel framework is meant to be used, i.e. where there is a large amount of unla-beled data but where it is only feasible to hand la-bel a relatively small portion of it. Accordingly, we applied the completed LFs to our \u201cunseen\u201d training set, 80% of the corpus, and used the final 10% as our test set. [FootnoteReference] 1 https://www.irit.fr/STAC/", "char_spans": [[1023, 1048, "URL"], [254, 265, "Name"], [530, 868, "Description"]], "id": 20, "role": "Material", "type": "Knowledge", "function": "Use", "url": "https://www.irit.fr/STAC/", "name": ["STAC corpus"], "fullname": ["N/A"], "genericmention": ["N/A"], "citationtag": ["N/A"], "description": ["Although the whole of the STAC corpus is annotated, we treated the remainder of the corpus as unseen/u-nannotated data in order to simulate the conditions in which the snorkel framework is meant to be used, i.e. where there is a large amount of unla-beled data but where it is only feasible to hand la-bel a relatively small portion of it"]}
{"text": "[SectionTitle] 5 Results and Analysis [Body] We first evaluated our LFs individually on the de-velopment corpus, which permitted us to measure their coverage and accuracy on a subset of the data [Cite_Footnote_3] . We then evaluated the generative model and the generative + discriminative model with the Snorkel architecture on the test set with the results in Table 2. [FootnoteReference] 3 https://tizirinagh.github.io/acl2019/", "char_spans": [[393, 430, "URL"], [45, 194, "Description"]], "id": 21, "role": "Material", "type": "Knowledge", "function": "Use", "url": "https://tizirinagh.github.io/acl2019/", "name": ["N/A"], "fullname": ["N/A"], "genericmention": ["N/A"], "citationtag": ["N/A"], "description": ["We first evaluated our LFs individually on the de-velopment corpus, which permitted us to measure their coverage and accuracy on a subset of the data"]}
{"text": "[SectionTitle] 1 Introduction [Body] We have conducted experimental comparisons on a widely used benchmark dataset ACE2005 [Cite_Footnote_1] . The results illustrate that our approach outper-forms all the compared baselines, and even achieves competitive performances compared with exiting approaches that used annotated triggers. We publish our code for further study by the NLP community. [FootnoteReference] 1 https://catalog.ldc.upenn.edu/LDC2006T06", "char_spans": [[413, 453, "URL"], [37, 122, "Description"], [115, 122, "Name"]], "id": 22, "role": "Material", "type": "Dataset", "function": "Compare", "url": "https://catalog.ldc.upenn.edu/LDC2006T06", "name": ["ACE2005"], "fullname": ["N/A"], "genericmention": ["N/A"], "citationtag": ["N/A"], "description": ["We have conducted experimental comparisons on a widely used benchmark dataset ACE2005"]}
{"text": "[SectionTitle] 1 Introduction [Body] We have conducted experimental comparisons on a widely used benchmark dataset ACE2005 . The results illustrate that our approach outper-forms all the compared baselines, and even achieves competitive performances compared with exiting approaches that used annotated triggers. We publish our code for further study by the NLP community. [Cite_Footnote_2] [FootnoteReference] 2 https://github.com/liushulinle/event detection without triggers", "char_spans": [[413, 449, "URL"], [328, 332, "Generic mention"], [125, 311, "Description"]], "id": 23, "role": "Method", "type": "Code", "function": "Produce", "url": "https://github.com/liushulinle/event", "name": ["N/A"], "fullname": ["N/A"], "genericmention": ["code"], "citationtag": ["N/A"], "description": ["The results illustrate that our approach outper-forms all the compared baselines, and even achieves competitive performances compared with exiting approaches that used annotated triggers"]}
{"text": "[SectionTitle] 3 Methodology 3.1 Input Tokens [Body] Given a sentence, we use Stanford CoreNLP tool-s [Cite_Footnote_3] (Manning et al., 2014) to convert texts into to-kens. The ACE 2005 corpus annotated not only events but also entities for each given sentence. Following previous work, we exploit the annotat-ed entity tags in our model(Li et al., 2013; Chen et al., 2015; Nguyen and Grishman, 2015, 2016; Liu et al., 2016b). [FootnoteReference] 3 http://stanfordnlp.github.io/CoreNLP", "char_spans": [[450, 486, "URL"], [53, 172, "Description"], [78, 101, "Name"], [121, 141, "Citation tag"]], "id": 24, "role": "Method", "type": "Tool", "function": "Use", "url": "http://stanfordnlp.github.io/CoreNLP", "name": ["Stanford CoreNLP tool-s"], "fullname": ["N/A"], "genericmention": ["N/A"], "citationtag": ["Manning et al., 2014"], "description": ["Given a sentence, we use Stanford CoreNLP tool-s [Cite_Footnote_3] (Manning et al., 2014) to convert texts into to-kens"]}
{"text": "[SectionTitle] 3 Methodology 3.2 Word/Entity Embeddings [Body] In this work, we use the Skip-gram mod-el(Mikolov et al., 2013) to learn word embeddings on the NYT corpus [Cite_Footnote_4] . Furthermore, we random-ly initialized an embedding table for each entity tags. All the input word tokens and entity tags will be transformed into low-dimensional vectors by looking up these embedding tables. In this work, we denote the dimension of word embeddings by d w , and that of entity embeddings by d e . [FootnoteReference] 4 https://catalog.ldc.upenn.edu/LDC2008T19", "char_spans": [[525, 565, "URL"], [77, 170, "Description"], [159, 169, "Name"]], "id": 25, "role": "Method", "type": "Code", "function": "Use", "url": "https://catalog.ldc.upenn.edu/LDC2008T19", "name": ["NYT corpus"], "fullname": ["N/A"], "genericmention": ["N/A"], "citationtag": ["N/A"], "description": ["we use the Skip-gram mod-el(Mikolov et al., 2013) to learn word embeddings on the NYT corpus "]}
{"text": "[SectionTitle] 4 Experiments 4.1 Datasets and Evaluation Metrics [Body] The first dataset, denoted as BOOK, is obtained from a popular Chinese book review website www. douban.com, which contains the descriptions of books and the tags collaboratively annotated by users. The second dataset, denoted as BIBTEX, is obtained from an English online bibliography web-site www.bibsonomy.org [Cite_Footnote_2] . The dataset contains the descriptions for academic papers (including the title and note for each paper) and the tags annotated by users. As shown in Table 2, the average length of descriptions in the BIBTEX dataset is much shorter than the BOOK dataset. Moreover, the BIBTEX dataset does not provide how many times each tag is annotated to a resource. [FootnoteReference] 2 The dataset can be obtained from http://www.kde.cs.uni-kassel.de/bibsonomy/dumps", "char_spans": [[811, 858, "URL"], [270, 383, "Description"], [281, 288, "Generic mention"], [778, 789, "Generic mention"]], "id": 26, "role": "Material", "type": "Dataset", "function": "Use", "url": "http://www.kde.cs.uni-kassel.de/bibsonomy/dumps", "name": ["N/A"], "fullname": ["N/A"], "genericmention": ["dataset", "The dataset"], "citationtag": ["N/A"], "description": ["The second dataset, denoted as BIBTEX, is obtained from an English online bibliography web-site www.bibsonomy.org"]}
{"text": "[SectionTitle] 2 System Architecture and Components 2.1 System Architecture [Body] ClaimPortal is composed of a front-end web based GUI, a MySQL database, an Elasticsearch [Cite_Footnote_3] search engine, an API, and several decoupled batch data processing components (Figure 1). The system operates on two layers. The front-end presentation layer allows users to narrow down search results by applying multiple filters. Keyword search on tweets is powered by Elasticsearch which is cou-pled with querying the database to provide addi-tional filters. Additionally, it provides numerous visualized graphs. The back-end data collection and computation layer performs pre-processing of tweets, computing check-worthiness scores of tweets using the public ClaimBuster API (Hassan et al., 2017a), Elasticsearch batch insertion, de-tecting claim types of tweets, and finding similar fact-checked claims for each tweet, using Claim-Buster API. ClaimPortal stays up-to-date with current tweets by periodically calling the Twitter REST API. [FootnoteReference] 3 https://www.elastic.co/products/elasticsearch", "char_spans": [[1054, 1099, "URL"], [83, 278, "Description"], [421, 549, "Description"]], "id": 27, "role": "Method", "type": "Tool", "function": "Use", "url": "https://www.elastic.co/products/elasticsearch", "name": ["N/A"], "fullname": ["N/A"], "genericmention": ["N/A"], "citationtag": ["N/A"], "description": ["ClaimPortal is composed of a front-end web based GUI, a MySQL database, an Elasticsearch [Cite_Footnote_3] search engine, an API, and several decoupled batch data processing components (Figure 1)", "Keyword search on tweets is powered by Elasticsearch which is cou-pled with querying the database to provide addi-tional filters"]}
{"text": "[SectionTitle] 2 System Architecture and Components 2.2 Monitoring, Processing, and Storing Tweets [Body] ClaimPortal\u2019s back-end layer focuses on data processing and storage. The Twitter REST API provides us with the necessary data. However, the system does not require all of it. In fact, a lot of the API\u2019s response is discarded to keep our database small and yet sufficient enough to pro-vide all necessary information for the portal. This is achieved through the ClaimPortal API. The API is a web service designed using Python and the Flask micro-framework. It provides end points for loading tweets on the GUI, search for hashtags, and search for users in applying from-user and user-mention filters. Based on the keyword search and filters requested by a user, the API queries the database to find the resulting list of tweet IDs and returns the list as a JSON response. A tweet ID is a unique number assigned to a tweet by Twitter. By using Twitter\u2019s card API [Cite_Footnote_5] the system dynami-cally populates the latest activity of a tweet at the front-end, based on its ID. [FootnoteReference] 5 https://developer.twitter.com/en/docs/tweets/optimize-with-cards", "char_spans": [[1107, 1171, "URL"], [939, 1084, "Description"], [948, 966, "Name"]], "id": 28, "role": "Method", "type": "Code", "function": "Introduce", "url": "https://developer.twitter.com/en/docs/tweets/optimize-with-cards", "name": ["Twitter\u2019s card API"], "fullname": ["N/A"], "genericmention": ["N/A"], "citationtag": ["N/A"], "description": ["By using Twitter\u2019s card API [Cite_Footnote_5] the system dynami-cally populates the latest activity of a tweet at the front-end, based on its ID."]}
{"text": "[SectionTitle] 2 System Architecture and Components 2.3 Claim Spotter [Body] In ClaimPortal, each tweet is given a check-worthiness score which denotes whether the tweet has a factual claim of which the truthfulness is im-portant to the public. This score is obtained by probing the ClaimBuster API, [Cite_Footnote_6] a well-known fact-checking tool, developed by our research group, that is being used by professional fact-checkers on a regular basis (Adair et al., 2019). Claim-Buster (Hassan et al., 2017a; Jimenez and Li, 2018) is a classification and ranking model trained on a human-labeled dataset of 8,000 sentences from past U.S. presidential debates. The Claim-Buster API returns a check-worthiness score for any given text. The score is on a scale from 0 to 1, ranging from least check-worthy to most check-worthy. The background task of probing Claim-Buster API for getting scores for tweets is another batch process, in parallel with the tweet collection and the Elasticsearch indexing processes. [FootnoteReference] 6 https://idir.uta.edu/claimbuster/", "char_spans": [[1032, 1065, "URL"], [283, 298, "Name"], [474, 659, "Description"], [488, 530, "Citation tag"], [474, 487, "Name"]], "id": 29, "role": "Method", "type": "Code", "function": "Use", "url": "https://idir.uta.edu/claimbuster/", "name": ["ClaimBuster API", "Claim-Buster "], "fullname": ["N/A"], "genericmention": ["N/A"], "citationtag": ["Hassan et al., 2017a; Jimenez and Li, 2018"], "description": ["Claim-Buster (Hassan et al., 2017a; Jimenez and Li, 2018) is a classification and ranking model trained on a human-labeled dataset of 8,000 sentences from past U.S. presidential debates"]}
